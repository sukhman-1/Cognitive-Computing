{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 9**"
      ],
      "metadata": {
        "id": "E1xtyYL7vEXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLVcCVBTt6CB",
        "outputId": "af39d65e-c27b-45ae-f268-68483de96e90"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:**"
      ],
      "metadata": {
        "id": "FHJE3W8fvHxU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emurPFhttNMe",
        "outputId": "2fa17c2a-fe25-4767-dc60-13fdfc1abbb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequency Distribution: Counter({'technology': 2, 'always': 1, 'fascinated': 1, 'first': 1, 'time': 1, 'used': 1, 'computer': 1, 'exploring': 1, 'cuttingedge': 1, 'ai': 1, 'passion': 1, 'grown': 1, 'love': 1, 'gadgets': 1, 'evolve': 1, 'rapidly': 1, 'connecting': 1, 'people': 1, 'across': 1, 'globe': 1, 'whether': 1, 'smartphones': 1, 'robots': 1, 'latest': 1, 'software': 1, 'never': 1, 'ceases': 1, 'amaze': 1, 'feels': 1, 'like': 1, 'living': 1, 'sciencefiction': 1, 'novel': 1, 'every': 1, 'single': 1, 'day': 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"Technology has always fascinated me. From the first time I used a computer to exploring cutting-edge AI, my passion has only grown. I love how gadgets evolve rapidly, connecting people across the globe. Whether it's smartphones, robots, or the latest software, technology never ceases to amaze me. It feels like living in a science-fiction novel every single day.\"\n",
        "\n",
        "text_lower = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "words = text_lower.split()\n",
        "sentences = text_lower.split('.')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [w for w in words if w not in stop_words]\n",
        "freq_dist = Counter(filtered_words)\n",
        "print(\"Word Frequency Distribution:\", freq_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**"
      ],
      "metadata": {
        "id": "W9s29DQ9vOOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "ls = LancasterStemmer()\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "porter_stemmed = [ps.stem(w) for w in filtered_words]\n",
        "lancaster_stemmed = [ls.stem(w) for w in filtered_words]\n",
        "lemmatized = [lem.lemmatize(w) for w in filtered_words]\n",
        "\n",
        "print(\"Porter Stemmer:\", porter_stemmed)\n",
        "print(\"Lancaster Stemmer:\", lancaster_stemmed)\n",
        "print(\"Lemmatization:\", lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbkZfEt1u5Dk",
        "outputId": "656dc701-8880-4b48-eda4-09051487bb9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['technolog', 'alway', 'fascin', 'first', 'time', 'use', 'comput', 'explor', 'cuttingedg', 'ai', 'passion', 'grown', 'love', 'gadget', 'evolv', 'rapidli', 'connect', 'peopl', 'across', 'globe', 'whether', 'smartphon', 'robot', 'latest', 'softwar', 'technolog', 'never', 'ceas', 'amaz', 'feel', 'like', 'live', 'sciencefict', 'novel', 'everi', 'singl', 'day']\n",
            "Lancaster Stemmer: ['technolog', 'alway', 'fascin', 'first', 'tim', 'us', 'comput', 'expl', 'cuttingedg', 'ai', 'pass', 'grown', 'lov', 'gadget', 'evolv', 'rapid', 'connect', 'peopl', 'across', 'glob', 'wheth', 'smartphon', 'robot', 'latest', 'softw', 'technolog', 'nev', 'ceas', 'amaz', 'feel', 'lik', 'liv', 'sciencefict', 'novel', 'every', 'singl', 'day']\n",
            "Lemmatization: ['technology', 'always', 'fascinated', 'first', 'time', 'used', 'computer', 'exploring', 'cuttingedge', 'ai', 'passion', 'grown', 'love', 'gadget', 'evolve', 'rapidly', 'connecting', 'people', 'across', 'globe', 'whether', 'smartphones', 'robot', 'latest', 'software', 'technology', 'never', 'cease', 'amaze', 'feel', 'like', 'living', 'sciencefiction', 'novel', 'every', 'single', 'day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**"
      ],
      "metadata": {
        "id": "9PWvsoLfvQ7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_more_than_5 = re.findall(r'\\b\\w{6,}\\b', text)\n",
        "numbers = re.findall(r'\\d+', text)\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "\n",
        "print(\"Words >5 letters:\", words_more_than_5)\n",
        "print(\"Numbers:\", numbers)\n",
        "print(\"Capitalized Words:\", capitalized_words)\n",
        "\n",
        "alpha_only = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "vowel_start = [w for w in alpha_only if w[0].lower() in 'aeiou']\n",
        "\n",
        "print(\"Alphabet Only Words:\", alpha_only)\n",
        "print(\"Words Starting with Vowel:\", vowel_start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ew-D10wvAZ9",
        "outputId": "f86e56ac-e28b-4efd-9787-913631a01714"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words >5 letters: ['Technology', 'always', 'fascinated', 'computer', 'exploring', 'cutting', 'passion', 'gadgets', 'evolve', 'rapidly', 'connecting', 'people', 'across', 'Whether', 'smartphones', 'robots', 'latest', 'software', 'technology', 'ceases', 'living', 'science', 'fiction', 'single']\n",
            "Numbers: []\n",
            "Capitalized Words: ['Technology', 'From', 'I', 'I', 'Whether', 'It']\n",
            "Alphabet Only Words: ['Technology', 'has', 'always', 'fascinated', 'me', 'From', 'the', 'first', 'time', 'I', 'used', 'a', 'computer', 'to', 'exploring', 'cutting', 'edge', 'AI', 'my', 'passion', 'has', 'only', 'grown', 'I', 'love', 'how', 'gadgets', 'evolve', 'rapidly', 'connecting', 'people', 'across', 'the', 'globe', 'Whether', 'it', 's', 'smartphones', 'robots', 'or', 'the', 'latest', 'software', 'technology', 'never', 'ceases', 'to', 'amaze', 'me', 'It', 'feels', 'like', 'living', 'in', 'a', 'science', 'fiction', 'novel', 'every', 'single', 'day']\n",
            "Words Starting with Vowel: ['always', 'I', 'used', 'a', 'exploring', 'edge', 'AI', 'only', 'I', 'evolve', 'across', 'it', 'or', 'amaze', 'It', 'in', 'a', 'every']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**"
      ],
      "metadata": {
        "id": "ivGYPQ_NvTFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tokenizer(t):\n",
        "    t = re.sub(r'(?<!\\w)[^\\w\\s\\'-]+|[^\\w\\s\\'-]+(?!\\w)', '', t)\n",
        "    t = re.sub(r'(\\d+\\.\\d+)|(\\d+)', r' \\1\\2 ', t)\n",
        "    tokens = t.split()\n",
        "    return tokens\n",
        "\n",
        "cleaned_text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b', '<EMAIL>', text)\n",
        "cleaned_text = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', cleaned_text)\n",
        "cleaned_text = re.sub(r'(\\+?\\d{1,3}[-\\s]?)?\\d{10}|\\d{3}-\\d{3}-\\d{4}', '<PHONE>', cleaned_text)\n",
        "\n",
        "custom_tokens = custom_tokenizer(cleaned_text)\n",
        "\n",
        "print(\"Custom Tokens:\", custom_tokens)\n",
        "print(\"Cleaned Text:\", cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5RZsu0XvV-3",
        "outputId": "7e23dadf-3732-434e-d5a3-e3c8fc800272"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokens: ['Technology', 'has', 'always', 'fascinated', 'me', 'From', 'the', 'first', 'time', 'I', 'used', 'a', 'computer', 'to', 'exploring', 'cutting-edge', 'AI', 'my', 'passion', 'has', 'only', 'grown', 'I', 'love', 'how', 'gadgets', 'evolve', 'rapidly', 'connecting', 'people', 'across', 'the', 'globe', 'Whether', \"it's\", 'smartphones', 'robots', 'or', 'the', 'latest', 'software', 'technology', 'never', 'ceases', 'to', 'amaze', 'me', 'It', 'feels', 'like', 'living', 'in', 'a', 'science-fiction', 'novel', 'every', 'single', 'day']\n",
            "Cleaned Text: Technology has always fascinated me. From the first time I used a computer to exploring cutting-edge AI, my passion has only grown. I love how gadgets evolve rapidly, connecting people across the globe. Whether it's smartphones, robots, or the latest software, technology never ceases to amaze me. It feels like living in a science-fiction novel every single day.\n"
          ]
        }
      ]
    }
  ]
}